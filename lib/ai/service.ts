import { OpenAI } from 'openai'
import { DEFAULT_PROMPTS, PromptOptimizer, VERIFIED_MODEL_LIST } from './config'

// åˆ›å»º OpenAI å®¢æˆ·ç«¯çš„å‡½æ•°
const createOpenAIClient = () => {
  const apiKey = process.env.OPENAI_API_KEY
  const baseURL = process.env.OPENAI_API_BASE_URL
  const model = process.env.OPENAI_API_MODEL || "openai/gpt-4.1-nano"
  
  if (!apiKey) {
    throw new Error('OPENAI_API_KEY environment variable is required')
  }
  
  console.log('Creating OpenAI client with config:', {
    baseURL,
    model,
    hasApiKey: !!apiKey
  })
  
  return new OpenAI({
    apiKey,
    baseURL,
    defaultHeaders: {
      'HTTP-Referer': 'https://game-admin.com',
      'X-Title': 'Game Admin'
    },
    defaultQuery: {
      model
    }
  })
}

// ä½¿ç”¨éªŒè¯è¿‡çš„4ä¸ªæ¨¡å‹ä½œä¸ºå¤‡ç”¨åˆ—è¡¨
const FALLBACK_MODELS = VERIFIED_MODEL_LIST

// è·å–å¯ç”¨çš„æ¨¡å‹ - ä¼˜åŒ– Edge Runtime å…¼å®¹æ€§
async function getAvailableModel(openai: OpenAI, preferredModel: string): Promise<string> {
  console.log(`Testing preferred model: ${preferredModel}`)
  
  // åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè·³è¿‡æ¨¡å‹æµ‹è¯•ç›´æ¥ä½¿ç”¨é¦–é€‰æ¨¡å‹
  // å› ä¸º Edge Runtime ä¸­çš„æµ‹è¯•è¯·æ±‚å¯èƒ½ä¼šå¤±è´¥ï¼Œä½†å®é™…ä½¿ç”¨æ—¶å¯èƒ½æ­£å¸¸
  if (process.env.NODE_ENV === 'production') {
    console.log(`ğŸš€ Production mode: Using preferred model ${preferredModel} without testing`)
    return preferredModel
  }
  
  // é¦–å…ˆæµ‹è¯•é¦–é€‰æ¨¡å‹
  try {
    const testResponse = await openai.chat.completions.create({
      model: preferredModel,
      messages: [{ role: 'user', content: 'Hello' }],
      max_tokens: 5,
      temperature: 0
    })
    
    if (testResponse.choices[0]?.message?.content) {
      console.log(`âœ… Preferred model ${preferredModel} is working`)
      return preferredModel
    }
  } catch (error) {
    console.log(`âŒ Preferred model ${preferredModel} failed:`, error instanceof Error ? error.message : 'Unknown error')
  }
  
  // è·å–ç¯å¢ƒå˜é‡é…ç½®çš„å¤‡ç”¨æ¨¡å‹
  const envFallbacks = [
    process.env.OPENAI_API_FALLBACK_MODEL_1,
    process.env.OPENAI_API_FALLBACK_MODEL_2,
    process.env.OPENAI_API_FALLBACK_MODEL_3
  ].filter(Boolean) as string[]
  
  // åˆå¹¶ç¯å¢ƒå˜é‡å¤‡ç”¨æ¨¡å‹å’Œé»˜è®¤å¤‡ç”¨æ¨¡å‹
  const allFallbacks = [
    ...envFallbacks,
    ...FALLBACK_MODELS.filter(m => 
      m !== preferredModel && 
      !envFallbacks.includes(m)
    )
  ]
  
  console.log('Fallback models:', { envFallbacks, totalFallbacks: allFallbacks.length })
  
  // åœ¨å¼€å‘ç¯å¢ƒä¸­æµ‹è¯•å¤‡ç”¨æ¨¡å‹
  for (const fallbackModel of allFallbacks) {
    if (fallbackModel === preferredModel) continue // è·³è¿‡å·²æµ‹è¯•çš„é¦–é€‰æ¨¡å‹
    
    try {
      console.log(`Testing fallback model: ${fallbackModel}`)
      const testResponse = await openai.chat.completions.create({
        model: fallbackModel,
        messages: [{ role: 'user', content: 'Hello' }],
        max_tokens: 5,
        temperature: 0
      })
      
      if (testResponse.choices[0]?.message?.content) {
        console.log(`âœ… Fallback model ${fallbackModel} is working`)
        return fallbackModel
      }
    } catch (error) {
      console.log(`âŒ Fallback model ${fallbackModel} failed:`, error instanceof Error ? error.message : 'Unknown error')
    }
  }
  
  // å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥ï¼Œä½†æˆ‘ä»¬åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½¿ç”¨é¦–é€‰æ¨¡å‹ä½œä¸ºæœ€åå°è¯•
  if (process.env.NODE_ENV === 'production') {
    console.log(`âš ï¸ All model tests failed, but using preferred model ${preferredModel} in production`)
    return preferredModel
  }
  
  throw new Error('No available models found. Please check your API configuration and model availability.')
}

// é‡è¯•é…ç½®
const MAX_RETRIES = 3
const BASE_DELAY = 1000 // 1 second

// å»¶è¿Ÿå‡½æ•°
const delay = async (attempt: number): Promise<void> => {
  const ms = Math.min(BASE_DELAY * Math.pow(2, attempt), 10000) // Max 10 seconds
  await new Promise(resolve => setTimeout(resolve, ms))
}

// é‡è¯•å‡½æ•°
const retryOperation = async <T>(
  operation: () => Promise<T>,
  attempt: number = 0
): Promise<T> => {
  try {
    return await operation()
  } catch (error) {
    console.error(`Attempt ${attempt + 1} failed:`, error)
    
    if (attempt < MAX_RETRIES - 1) {
      await delay(attempt)
      return retryOperation(operation, attempt + 1)
    }
    
    // å¦‚æœæ˜¯è¿æ¥é”™è¯¯ï¼Œæä¾›æ›´å…·ä½“çš„é”™è¯¯ä¿¡æ¯
    if (error instanceof Error && error.message.includes('ECONNRESET')) {
      throw new Error('Failed to connect to AI service. Please check your network connection and try again.')
    }
    
    throw error
  }
}

// éªŒè¯URLæ˜¯å¦è¢«ä¿®æ”¹
const validateUrlPreservation = (originalUrl: string, processedUrl: string): string => {
  if (!originalUrl || !processedUrl) return processedUrl
  
  // å¦‚æœåŸå§‹URLåŒ…å«è¿å­—ç¬¦ï¼Œä½†å¤„ç†åå˜æˆäº†ä¸‹åˆ’çº¿ï¼Œæ¢å¤åŸå§‹æ ¼å¼
  if (originalUrl.includes('-') && processedUrl.includes('_')) {
    console.warn('URL format was modified, restoring original format')
    return originalUrl
  }
  
  return processedUrl
}

// å¤„ç†å›¾ç‰‡ URL æ ¼å¼
const processImageUrl = (url: string): string => {
  if (!url) return ''
  if (url.startsWith('http://') || url.startsWith('https://')) {
    return url
  }
  if (url.startsWith('/')) {
    return url
  }
  return `https://${url}`
}

// æ¸…ç†å’Œè§£æ AI å“åº”
const parseAIResponse = (content: string): any => {
  if (!content) {
    throw new Error('Empty response from AI')
  }

  console.log('Raw AI response:', content)

  try {
    // é¦–å…ˆå°è¯•ç›´æ¥è§£æ JSON
    return JSON.parse(content)
  } catch (error) {
    console.log('Direct JSON parsing failed, trying to clean content...')
    
    // æ¸…ç†å†…å®¹ä¸­çš„ Markdown ä»£ç å—æ ‡è®°
    let cleanContent = content
      .replace(/^```json\s*/i, '')
      .replace(/^```\s*/i, '')
      .replace(/\s*```$/i, '')
      .trim()
    
    console.log('Cleaned content:', cleanContent)
    
    try {
      return JSON.parse(cleanContent)
    } catch (secondError) {
      console.error('Failed to parse cleaned content:', secondError)
      console.error('Content that failed to parse:', cleanContent)
      
      // å°è¯•æå– JSON éƒ¨åˆ†
      const jsonMatch = cleanContent.match(/\{[\s\S]*\}/)
      if (jsonMatch) {
        try {
          return JSON.parse(jsonMatch[0])
        } catch (thirdError) {
          console.error('Failed to parse extracted JSON:', thirdError)
        }
      }
      
      throw new Error(`Invalid JSON response from AI. Raw content: ${content.substring(0, 200)}...`)
    }
  }
}

// ç”Ÿæˆå†…å®¹
export const generateContent = async (
  input: string,
  taskType: 'GAME_IMPORT' | 'GAME_CONTENT',
  customPrompt?: string,
  temperature: number = 0.7
) => {
  const systemPrompt = customPrompt || DEFAULT_PROMPTS[taskType] 
  if (!systemPrompt) {
    throw new Error(`Invalid task type: ${taskType}`)
  }

  return retryOperation(async () => {
    console.log('Sending request to OpenAI API...')
    console.log('Model:', process.env.OPENAI_API_MODEL)
    console.log('Base URL:', process.env.OPENAI_API_BASE_URL)
    console.log('Task type:', taskType)

    try {
      const openai = createOpenAIClient()
      
      // è·å–å¯ç”¨çš„æ¨¡å‹
      const availableModel = await getAvailableModel(openai, process.env.OPENAI_API_MODEL || "openai/gpt-4.1-nano")
      console.log('Using model:', availableModel)
      
      // ä½¿ç”¨æç¤ºè¯ä¼˜åŒ–å™¨
      const optimizedPrompt = PromptOptimizer.optimizePrompt(taskType, { model: availableModel })
      const modelConfig = PromptOptimizer.getModelConfig(availableModel)
      
      const completion = await openai.chat.completions.create({
        model: availableModel,
        messages: [
          { role: "system", content: optimizedPrompt },
          { role: "user", content: input }
        ],
        response_format: { type: "json_object" },
        temperature: modelConfig.temperature,
        max_tokens: modelConfig.maxTokens,
        top_p: modelConfig.topP,
        frequency_penalty: modelConfig.frequencyPenalty
      })

      console.log('OpenAI API Response received')
      console.log('Response usage:', completion.usage)
      console.log('Response choices:', completion.choices.length)
      console.log('First choice:', completion.choices[0])
      console.log('Message:', completion.choices[0]?.message)

      const content = completion.choices[0]?.message?.content
      console.log('Content:', content)
      console.log('Content length:', content?.length || 0)
      console.log('Finish reason:', completion.choices[0]?.finish_reason)
      
      if (!content) {
        console.error('Empty response details:', {
          choices: completion.choices.length,
          firstChoice: completion.choices[0],
          message: completion.choices[0]?.message,
          usage: completion.usage
        })
        throw new Error('Empty response from AI service - model may be unavailable or overloaded')
      }

      const parsedContent = parseAIResponse(content)

      // å¤„ç†å›¾ç‰‡ URL å¹¶éªŒè¯æ ¼å¼
      if (parsedContent.imageUrl) {
        const processedUrl = processImageUrl(parsedContent.imageUrl)
        // å°è¯•ä»è¾“å…¥ä¸­æå–åŸå§‹imageUrlè¿›è¡ŒéªŒè¯
        try {
          const inputData = JSON.parse(input)
          if (inputData.imageUrl) {
            parsedContent.imageUrl = validateUrlPreservation(inputData.imageUrl, processedUrl)
          } else {
            parsedContent.imageUrl = processedUrl
          }
        } catch {
          parsedContent.imageUrl = processedUrl
        }
      }
      // videoå­—æ®µå·²è¢«ç§»é™¤

      console.log('Successfully processed content')
      return parsedContent
    } catch (error) {
      console.error('Error in generateContent:', error)
      
      // æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
      if (error instanceof Error) {
        if (error.message.includes('rate limit')) {
          throw new Error('AI service rate limit exceeded. Please try again in a few minutes.')
        } else if (error.message.includes('quota')) {
          throw new Error('AI service quota exceeded. Please check your API limits.')
        } else if (error.message.includes('timeout')) {
          throw new Error('AI service request timed out. Please try again.')
        } else if (error.message.includes('No available models found')) {
          throw new Error('AI service temporarily unavailable. Please try again in a moment.')
        } else if (error.message.includes('Invalid request')) {
          throw new Error('Invalid AI request. Please check your input data.')
        } else if (error.message.includes('Unauthorized')) {
          throw new Error('AI service authentication failed. Please check API configuration.')
        }
      }
      
      throw error
    }
  })
}

// æ¸¸æˆæ•°æ®ç”Ÿæˆ
export const generateGameData = async (
  rawData: string,
  customPrompt?: string
) => {
  // æ·»åŠ é¢å¤–çš„URLä¿æŠ¤æç¤º
  const enhancedPrompt = customPrompt || DEFAULT_PROMPTS['GAME_IMPORT']
  const finalPrompt = enhancedPrompt + `

âš ï¸ **FINAL REMINDER** âš ï¸
The imageUrl and iframeUrl in your response MUST be copied EXACTLY from the input data.
Do not change any characters, especially hyphens (-) to underscores (_).
If input has "bunny-farm", output must have "bunny-farm", not "bunny_farm".`

  return generateContent(rawData, 'GAME_IMPORT', finalPrompt)
}

// æ¸¸æˆå†…å®¹ç”Ÿæˆ
export const createGameContent = async (
  gameInfo: string,
  customPrompt?: string
) => {
  return generateContent(gameInfo, 'GAME_CONTENT', customPrompt)
} 